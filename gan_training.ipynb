{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7739dfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "torch.use_deterministic_algorithms(True) # Needed for reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3ab401d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thiết bị sử dụng: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Thiết bị sử dụng: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c652f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "LATENT_DIM = 100    \n",
    "LEARNING_RATE = 0.0002 \n",
    "NUM_EPOCHS = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e4f1d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>frame</th>\n",
       "      <th>x0</th>\n",
       "      <th>y0</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>...</th>\n",
       "      <th>x16</th>\n",
       "      <th>y16</th>\n",
       "      <th>x17</th>\n",
       "      <th>y17</th>\n",
       "      <th>x18</th>\n",
       "      <th>y18</th>\n",
       "      <th>x19</th>\n",
       "      <th>y19</th>\n",
       "      <th>x20</th>\n",
       "      <th>y20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thumb</td>\n",
       "      <td>0</td>\n",
       "      <td>0.599915</td>\n",
       "      <td>0.665262</td>\n",
       "      <td>0.515953</td>\n",
       "      <td>0.665098</td>\n",
       "      <td>0.428840</td>\n",
       "      <td>0.633735</td>\n",
       "      <td>0.373814</td>\n",
       "      <td>0.601213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.564590</td>\n",
       "      <td>0.550839</td>\n",
       "      <td>0.594947</td>\n",
       "      <td>0.469473</td>\n",
       "      <td>0.579057</td>\n",
       "      <td>0.432039</td>\n",
       "      <td>0.587348</td>\n",
       "      <td>0.481080</td>\n",
       "      <td>0.593778</td>\n",
       "      <td>0.516935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>thumb</td>\n",
       "      <td>1</td>\n",
       "      <td>0.603023</td>\n",
       "      <td>0.697286</td>\n",
       "      <td>0.507298</td>\n",
       "      <td>0.672530</td>\n",
       "      <td>0.430429</td>\n",
       "      <td>0.638797</td>\n",
       "      <td>0.374489</td>\n",
       "      <td>0.601450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563151</td>\n",
       "      <td>0.544787</td>\n",
       "      <td>0.603814</td>\n",
       "      <td>0.477666</td>\n",
       "      <td>0.586528</td>\n",
       "      <td>0.427248</td>\n",
       "      <td>0.588528</td>\n",
       "      <td>0.476918</td>\n",
       "      <td>0.594982</td>\n",
       "      <td>0.515491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thumb</td>\n",
       "      <td>2</td>\n",
       "      <td>0.578484</td>\n",
       "      <td>0.664635</td>\n",
       "      <td>0.484652</td>\n",
       "      <td>0.639102</td>\n",
       "      <td>0.405413</td>\n",
       "      <td>0.603161</td>\n",
       "      <td>0.334007</td>\n",
       "      <td>0.565351</td>\n",
       "      <td>...</td>\n",
       "      <td>0.531455</td>\n",
       "      <td>0.492580</td>\n",
       "      <td>0.565373</td>\n",
       "      <td>0.441291</td>\n",
       "      <td>0.561908</td>\n",
       "      <td>0.379894</td>\n",
       "      <td>0.566244</td>\n",
       "      <td>0.439990</td>\n",
       "      <td>0.565475</td>\n",
       "      <td>0.475714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thumb</td>\n",
       "      <td>3</td>\n",
       "      <td>0.581487</td>\n",
       "      <td>0.675659</td>\n",
       "      <td>0.482386</td>\n",
       "      <td>0.638060</td>\n",
       "      <td>0.405132</td>\n",
       "      <td>0.608401</td>\n",
       "      <td>0.334031</td>\n",
       "      <td>0.565769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.531379</td>\n",
       "      <td>0.501897</td>\n",
       "      <td>0.565993</td>\n",
       "      <td>0.438266</td>\n",
       "      <td>0.559864</td>\n",
       "      <td>0.379514</td>\n",
       "      <td>0.561746</td>\n",
       "      <td>0.442510</td>\n",
       "      <td>0.562781</td>\n",
       "      <td>0.481083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thumb</td>\n",
       "      <td>4</td>\n",
       "      <td>0.551713</td>\n",
       "      <td>0.643821</td>\n",
       "      <td>0.452903</td>\n",
       "      <td>0.613882</td>\n",
       "      <td>0.370420</td>\n",
       "      <td>0.581506</td>\n",
       "      <td>0.294350</td>\n",
       "      <td>0.545547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.499876</td>\n",
       "      <td>0.477000</td>\n",
       "      <td>0.539230</td>\n",
       "      <td>0.416825</td>\n",
       "      <td>0.530130</td>\n",
       "      <td>0.364053</td>\n",
       "      <td>0.533174</td>\n",
       "      <td>0.430488</td>\n",
       "      <td>0.532756</td>\n",
       "      <td>0.462243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  frame        x0        y0        x1        y1        x2        y2  \\\n",
       "0  thumb      0  0.599915  0.665262  0.515953  0.665098  0.428840  0.633735   \n",
       "1  thumb      1  0.603023  0.697286  0.507298  0.672530  0.430429  0.638797   \n",
       "2  thumb      2  0.578484  0.664635  0.484652  0.639102  0.405413  0.603161   \n",
       "3  thumb      3  0.581487  0.675659  0.482386  0.638060  0.405132  0.608401   \n",
       "4  thumb      4  0.551713  0.643821  0.452903  0.613882  0.370420  0.581506   \n",
       "\n",
       "         x3        y3  ...       x16       y16       x17       y17       x18  \\\n",
       "0  0.373814  0.601213  ...  0.564590  0.550839  0.594947  0.469473  0.579057   \n",
       "1  0.374489  0.601450  ...  0.563151  0.544787  0.603814  0.477666  0.586528   \n",
       "2  0.334007  0.565351  ...  0.531455  0.492580  0.565373  0.441291  0.561908   \n",
       "3  0.334031  0.565769  ...  0.531379  0.501897  0.565993  0.438266  0.559864   \n",
       "4  0.294350  0.545547  ...  0.499876  0.477000  0.539230  0.416825  0.530130   \n",
       "\n",
       "        y18       x19       y19       x20       y20  \n",
       "0  0.432039  0.587348  0.481080  0.593778  0.516935  \n",
       "1  0.427248  0.588528  0.476918  0.594982  0.515491  \n",
       "2  0.379894  0.566244  0.439990  0.565475  0.475714  \n",
       "3  0.379514  0.561746  0.442510  0.562781  0.481083  \n",
       "4  0.364053  0.533174  0.430488  0.532756  0.462243  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('all_landmarks.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "47e14f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'x0', 'y0', 'x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4',\n",
       "       'x5', 'y5', 'x6', 'y6', 'x7', 'y7', 'x8', 'y8', 'x9', 'y9', 'x10',\n",
       "       'y10', 'x11', 'y11', 'x12', 'y12', 'x13', 'y13', 'x14', 'y14', 'x15',\n",
       "       'y15', 'x16', 'y16', 'x17', 'y17', 'x18', 'y18', 'x19', 'y19', 'x20',\n",
       "       'y20'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=['frame'])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cf03aa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng lớp (nhãn dán): 2\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = df['label'].nunique()\n",
    "print(f\"Số lượng lớp (nhãn dán): {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "192c8097",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoordinateDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Khởi tạo dataset từ file CSV.\n",
    "        Args:\n",
    "            csv_file (str): Đường dẫn đến file CSV chứa tọa độ và nhãn dán.\n",
    "        \"\"\"\n",
    "        self.data = df\n",
    "        \n",
    "        # Lấy nhãn dán\n",
    "        self.labels = self.data['label'].values.reshape(-1, 1)\n",
    "\n",
    "        # Sử dụng LabelEncoder và OneHotEncoder\n",
    "        # 1. Label Encoding: Chuyển đổi nhãn chữ thành số\n",
    "        le = LabelEncoder()\n",
    "        integer_encoded = le.fit_transform(self.labels.ravel())\n",
    "        \n",
    "        # 2. One-Hot Encoding: Chuyển đổi số thành vector nhị phân\n",
    "        onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "        self.onehot_labels = onehot_encoder.fit_transform(integer_encoded.reshape(-1, 1))\n",
    "\n",
    "        # Lưu thông tin về các nhãn\n",
    "        self.num_classes = self.onehot_labels.shape[1]\n",
    "        self.label_to_int = {label: i for i, label in enumerate(le.classes_)}\n",
    "        self.int_to_label = {i: label for i, label in enumerate(le.classes_)}\n",
    "\n",
    "        # Lấy dữ liệu tọa độ\n",
    "        self.coordinates = self.data.drop('label', axis=1).values.astype(np.float32)\n",
    "\n",
    "        # Lưu lại min/max của tọa độ để khôi phục sau này\n",
    "        self.coordinates_min = self.coordinates.min(axis=0)\n",
    "        self.coordinates_max = self.coordinates.max(axis=0)\n",
    "\n",
    "        # Chuẩn hóa dữ liệu tọa độ về khoảng [-1, 1]\n",
    "        self.coordinates = 2 * (self.coordinates - self.coordinates_min) / (self.coordinates_max - self.coordinates_min) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Trả về số lượng mẫu trong dataset.\n",
    "        \"\"\"\n",
    "        return len(self.coordinates)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Lấy một mẫu dữ liệu và nhãn dán tại chỉ số idx.\n",
    "        \"\"\"\n",
    "        coordinates = torch.tensor(self.coordinates[idx], dtype=torch.float32)\n",
    "        onehot_label = torch.tensor(self.onehot_labels[idx], dtype=torch.float32)\n",
    "        return coordinates, onehot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "053ee75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng đặc trưng (điểm tọa độ): 42\n"
     ]
    }
   ],
   "source": [
    "# Đổi thành đường dẫn tới file CSV của bạn\n",
    "dataset = CoordinateDataset(df)\n",
    "dataloader = DataLoader(dataset, \n",
    "                        batch_size=BATCH_SIZE, \n",
    "                        shuffle=True)\n",
    "\n",
    "# Lấy số lượng đặc trưng từ dữ liệu\n",
    "NUM_FEARTURES = len(dataset[0][0])\n",
    "print(f\"Số lượng đặc trưng (điểm tọa độ): {NUM_FEARTURES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a93439b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, num_features, num_classes):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Lớp đầu vào nhận cả latent_dim và num_classes\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim + self.num_classes, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, num_features),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z, labels):\n",
    "        # Nối vector nhiễu (z) và vector nhãn (labels)\n",
    "        conditional_input = torch.cat([z, labels], 1)\n",
    "        return self.main(conditional_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "58598f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Lớp đầu vào nhận cả num_features và num_classes\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(num_features + self.num_classes, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        # Nối dữ liệu (x) và vector nhãn (labels)\n",
    "        conditional_input = torch.cat([x, labels], 1)\n",
    "        return self.main(conditional_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "27507534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo các mô hình và chuyển sang device\n",
    "generator = Generator(LATENT_DIM, NUM_FEARTURES, NUM_CLASSES).to(device)\n",
    "discriminator = Discriminator(NUM_FEARTURES, NUM_CLASSES).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a5704618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Batch 0/157                   Loss D: 1.3841, loss G: 0.6980                   D(x): 0.4995, D(G(z)): 0.4984 / 0.4976\n",
      "Epoch [1/100] Batch 50/157                   Loss D: 0.8271, loss G: 1.5367                   D(x): 0.7442, D(G(z)): 0.3742 / 0.2177\n",
      "Epoch [1/100] Batch 100/157                   Loss D: 1.9246, loss G: 1.0798                   D(x): 0.5591, D(G(z)): 0.6876 / 0.3476\n",
      "Epoch [1/100] Batch 150/157                   Loss D: 0.8141, loss G: 1.9995                   D(x): 0.6296, D(G(z)): 0.2647 / 0.1362\n",
      "Epoch [2/100] Batch 0/157                   Loss D: 0.5634, loss G: 2.3775                   D(x): 0.7360, D(G(z)): 0.2055 / 0.0940\n",
      "Epoch [2/100] Batch 50/157                   Loss D: 0.2966, loss G: 2.6274                   D(x): 0.8779, D(G(z)): 0.1420 / 0.0730\n",
      "Epoch [2/100] Batch 100/157                   Loss D: 0.5631, loss G: 2.8197                   D(x): 0.8164, D(G(z)): 0.2551 / 0.0660\n",
      "Epoch [2/100] Batch 150/157                   Loss D: 0.6976, loss G: 2.7442                   D(x): 0.8596, D(G(z)): 0.3710 / 0.0701\n",
      "Epoch [3/100] Batch 0/157                   Loss D: 1.6698, loss G: 1.5101                   D(x): 0.7702, D(G(z)): 0.7091 / 0.2313\n",
      "Epoch [3/100] Batch 50/157                   Loss D: 0.8472, loss G: 2.7807                   D(x): 0.6973, D(G(z)): 0.2414 / 0.0645\n",
      "Epoch [3/100] Batch 100/157                   Loss D: 1.0446, loss G: 2.7640                   D(x): 0.7253, D(G(z)): 0.2709 / 0.0655\n",
      "Epoch [3/100] Batch 150/157                   Loss D: 1.7877, loss G: 0.7930                   D(x): 0.7998, D(G(z)): 0.7529 / 0.4656\n",
      "Epoch [4/100] Batch 0/157                   Loss D: 0.5730, loss G: 3.0329                   D(x): 0.7973, D(G(z)): 0.2020 / 0.0500\n",
      "Epoch [4/100] Batch 50/157                   Loss D: 1.0561, loss G: 2.5029                   D(x): 0.6459, D(G(z)): 0.2037 / 0.0842\n",
      "Epoch [4/100] Batch 100/157                   Loss D: 0.7762, loss G: 2.6419                   D(x): 0.8034, D(G(z)): 0.2699 / 0.0779\n",
      "Epoch [4/100] Batch 150/157                   Loss D: 0.8440, loss G: 2.8636                   D(x): 0.7625, D(G(z)): 0.3036 / 0.0591\n",
      "Epoch [5/100] Batch 0/157                   Loss D: 0.8051, loss G: 2.3190                   D(x): 0.7408, D(G(z)): 0.1099 / 0.1021\n",
      "Epoch [5/100] Batch 50/157                   Loss D: 1.0661, loss G: 2.3583                   D(x): 0.7636, D(G(z)): 0.4271 / 0.1020\n",
      "Epoch [5/100] Batch 100/157                   Loss D: 1.0770, loss G: 2.5797                   D(x): 0.7341, D(G(z)): 0.3126 / 0.0890\n",
      "Epoch [5/100] Batch 150/157                   Loss D: 5.2748, loss G: 0.2146                   D(x): 0.7340, D(G(z)): 0.9732 / 0.8184\n",
      "Epoch [6/100] Batch 0/157                   Loss D: 3.5062, loss G: 2.2997                   D(x): 0.4612, D(G(z)): 0.5869 / 0.1083\n",
      "Epoch [6/100] Batch 50/157                   Loss D: 0.6593, loss G: 2.7746                   D(x): 0.8669, D(G(z)): 0.2860 / 0.0741\n",
      "Epoch [6/100] Batch 100/157                   Loss D: 0.9806, loss G: 1.9213                   D(x): 0.8492, D(G(z)): 0.5191 / 0.1592\n",
      "Epoch [6/100] Batch 150/157                   Loss D: 0.9175, loss G: 3.2304                   D(x): 0.7266, D(G(z)): 0.2321 / 0.0427\n",
      "Epoch [7/100] Batch 0/157                   Loss D: 0.8725, loss G: 2.8769                   D(x): 0.7187, D(G(z)): 0.1702 / 0.0632\n",
      "Epoch [7/100] Batch 50/157                   Loss D: 0.1228, loss G: 4.5957                   D(x): 0.9228, D(G(z)): 0.0317 / 0.0109\n",
      "Epoch [7/100] Batch 100/157                   Loss D: 0.0853, loss G: 4.4535                   D(x): 0.9457, D(G(z)): 0.0257 / 0.0130\n",
      "Epoch [7/100] Batch 150/157                   Loss D: 0.0835, loss G: 3.8169                   D(x): 0.9467, D(G(z)): 0.0226 / 0.0290\n",
      "Epoch [8/100] Batch 0/157                   Loss D: 0.6395, loss G: 1.5005                   D(x): 0.7303, D(G(z)): 0.0502 / 0.2614\n",
      "Epoch [8/100] Batch 50/157                   Loss D: 0.9837, loss G: 3.4866                   D(x): 0.7669, D(G(z)): 0.2911 / 0.0389\n",
      "Epoch [8/100] Batch 100/157                   Loss D: 2.5388, loss G: 2.4390                   D(x): 0.4679, D(G(z)): 0.2885 / 0.0901\n",
      "Epoch [8/100] Batch 150/157                   Loss D: 0.5799, loss G: 3.2961                   D(x): 0.8235, D(G(z)): 0.2101 / 0.0434\n",
      "Epoch [9/100] Batch 0/157                   Loss D: 0.7759, loss G: 3.3683                   D(x): 0.8491, D(G(z)): 0.1942 / 0.0475\n",
      "Epoch [9/100] Batch 50/157                   Loss D: 0.5600, loss G: 3.2611                   D(x): 0.7965, D(G(z)): 0.1745 / 0.0444\n",
      "Epoch [9/100] Batch 100/157                   Loss D: 1.8491, loss G: 2.3296                   D(x): 0.6690, D(G(z)): 0.3362 / 0.1315\n",
      "Epoch [9/100] Batch 150/157                   Loss D: 0.9830, loss G: 1.9702                   D(x): 0.8087, D(G(z)): 0.3923 / 0.1773\n",
      "Epoch [10/100] Batch 0/157                   Loss D: 1.1007, loss G: 2.1705                   D(x): 0.7007, D(G(z)): 0.3217 / 0.1419\n",
      "Epoch [10/100] Batch 50/157                   Loss D: 0.7744, loss G: 2.0370                   D(x): 0.7418, D(G(z)): 0.1802 / 0.1450\n",
      "Epoch [10/100] Batch 100/157                   Loss D: 0.2660, loss G: 2.2994                   D(x): 0.8535, D(G(z)): 0.0802 / 0.1218\n",
      "Epoch [10/100] Batch 150/157                   Loss D: 0.8430, loss G: 2.7767                   D(x): 0.8108, D(G(z)): 0.4089 / 0.0783\n",
      "Epoch [11/100] Batch 0/157                   Loss D: 0.6133, loss G: 1.3287                   D(x): 0.7374, D(G(z)): 0.1700 / 0.2894\n",
      "Epoch [11/100] Batch 50/157                   Loss D: 0.8228, loss G: 2.6462                   D(x): 0.8982, D(G(z)): 0.4463 / 0.0786\n",
      "Epoch [11/100] Batch 100/157                   Loss D: 0.5788, loss G: 3.1598                   D(x): 0.7951, D(G(z)): 0.1922 / 0.0520\n",
      "Epoch [11/100] Batch 150/157                   Loss D: 1.0483, loss G: 3.3993                   D(x): 0.8806, D(G(z)): 0.4686 / 0.0713\n",
      "Epoch [12/100] Batch 0/157                   Loss D: 0.9798, loss G: 1.3465                   D(x): 0.6547, D(G(z)): 0.1036 / 0.3330\n",
      "Epoch [12/100] Batch 50/157                   Loss D: 0.8824, loss G: 1.0628                   D(x): 0.6342, D(G(z)): 0.1793 / 0.3870\n",
      "Epoch [12/100] Batch 100/157                   Loss D: 0.6405, loss G: 1.6974                   D(x): 0.7066, D(G(z)): 0.1059 / 0.2527\n",
      "Epoch [12/100] Batch 150/157                   Loss D: 1.1192, loss G: 1.9480                   D(x): 0.8749, D(G(z)): 0.5640 / 0.2032\n",
      "Epoch [13/100] Batch 0/157                   Loss D: 1.0452, loss G: 0.8462                   D(x): 0.5144, D(G(z)): 0.1674 / 0.4881\n",
      "Epoch [13/100] Batch 50/157                   Loss D: 0.9202, loss G: 0.7331                   D(x): 0.6013, D(G(z)): 0.2110 / 0.4994\n",
      "Epoch [13/100] Batch 100/157                   Loss D: 0.7094, loss G: 0.7202                   D(x): 0.6567, D(G(z)): 0.1794 / 0.5315\n",
      "Epoch [13/100] Batch 150/157                   Loss D: 0.8502, loss G: 0.9314                   D(x): 0.5490, D(G(z)): 0.1420 / 0.4521\n",
      "Epoch [14/100] Batch 0/157                   Loss D: 0.9495, loss G: 2.2327                   D(x): 0.8317, D(G(z)): 0.4688 / 0.1377\n",
      "Epoch [14/100] Batch 50/157                   Loss D: 0.9618, loss G: 2.3043                   D(x): 0.8826, D(G(z)): 0.5267 / 0.1361\n",
      "Epoch [14/100] Batch 100/157                   Loss D: 0.9832, loss G: 2.3609                   D(x): 0.8621, D(G(z)): 0.5201 / 0.1166\n",
      "Epoch [14/100] Batch 150/157                   Loss D: 1.0331, loss G: 2.1623                   D(x): 0.8416, D(G(z)): 0.5339 / 0.1293\n",
      "Epoch [15/100] Batch 0/157                   Loss D: 0.9432, loss G: 0.7485                   D(x): 0.6058, D(G(z)): 0.1163 / 0.4982\n",
      "Epoch [15/100] Batch 50/157                   Loss D: 0.9551, loss G: 0.9753                   D(x): 0.5929, D(G(z)): 0.1363 / 0.4502\n",
      "Epoch [15/100] Batch 100/157                   Loss D: 0.8565, loss G: 0.9049                   D(x): 0.6398, D(G(z)): 0.1180 / 0.4468\n",
      "Epoch [15/100] Batch 150/157                   Loss D: 0.7282, loss G: 1.0592                   D(x): 0.7287, D(G(z)): 0.1140 / 0.3936\n",
      "Epoch [16/100] Batch 0/157                   Loss D: 0.7329, loss G: 3.0882                   D(x): 0.7805, D(G(z)): 0.3005 / 0.0700\n",
      "Epoch [16/100] Batch 50/157                   Loss D: 1.0259, loss G: 2.5431                   D(x): 0.8299, D(G(z)): 0.5013 / 0.1025\n",
      "Epoch [16/100] Batch 100/157                   Loss D: 0.9130, loss G: 2.6342                   D(x): 0.8633, D(G(z)): 0.4971 / 0.1026\n",
      "Epoch [16/100] Batch 150/157                   Loss D: 0.9313, loss G: 2.4922                   D(x): 0.8572, D(G(z)): 0.5045 / 0.1093\n",
      "Epoch [17/100] Batch 0/157                   Loss D: 0.9101, loss G: 1.1303                   D(x): 0.6770, D(G(z)): 0.1874 / 0.3562\n",
      "Epoch [17/100] Batch 50/157                   Loss D: 0.8086, loss G: 1.0268                   D(x): 0.6897, D(G(z)): 0.1419 / 0.4086\n",
      "Epoch [17/100] Batch 100/157                   Loss D: 0.8281, loss G: 0.8145                   D(x): 0.6810, D(G(z)): 0.1201 / 0.4706\n",
      "Epoch [17/100] Batch 150/157                   Loss D: 0.9288, loss G: 0.7282                   D(x): 0.6184, D(G(z)): 0.1147 / 0.5088\n",
      "Epoch [18/100] Batch 0/157                   Loss D: 0.8963, loss G: 2.5263                   D(x): 0.8599, D(G(z)): 0.4985 / 0.1048\n",
      "Epoch [18/100] Batch 50/157                   Loss D: 0.9288, loss G: 2.5741                   D(x): 0.8374, D(G(z)): 0.4901 / 0.1052\n",
      "Epoch [18/100] Batch 100/157                   Loss D: 1.0085, loss G: 2.3827                   D(x): 0.8320, D(G(z)): 0.4952 / 0.1099\n",
      "Epoch [18/100] Batch 150/157                   Loss D: 1.0303, loss G: 2.2962                   D(x): 0.8792, D(G(z)): 0.5453 / 0.1203\n",
      "Epoch [19/100] Batch 0/157                   Loss D: 0.8513, loss G: 0.7269                   D(x): 0.6231, D(G(z)): 0.1117 / 0.4996\n",
      "Epoch [19/100] Batch 50/157                   Loss D: 0.9915, loss G: 0.8858                   D(x): 0.6014, D(G(z)): 0.1052 / 0.4465\n",
      "Epoch [19/100] Batch 100/157                   Loss D: 0.8002, loss G: 0.7571                   D(x): 0.6441, D(G(z)): 0.1278 / 0.5067\n",
      "Epoch [19/100] Batch 150/157                   Loss D: 0.8415, loss G: 0.9383                   D(x): 0.6266, D(G(z)): 0.1096 / 0.4105\n",
      "Epoch [20/100] Batch 0/157                   Loss D: 0.7747, loss G: 2.5294                   D(x): 0.7919, D(G(z)): 0.3328 / 0.1116\n",
      "Epoch [20/100] Batch 50/157                   Loss D: 0.9079, loss G: 2.6777                   D(x): 0.8157, D(G(z)): 0.4443 / 0.0872\n",
      "Epoch [20/100] Batch 100/157                   Loss D: 0.9903, loss G: 2.4366                   D(x): 0.8308, D(G(z)): 0.5122 / 0.1071\n",
      "Epoch [20/100] Batch 150/157                   Loss D: 0.9599, loss G: 2.4905                   D(x): 0.8233, D(G(z)): 0.4997 / 0.0980\n",
      "Epoch [21/100] Batch 0/157                   Loss D: 0.8093, loss G: 0.6946                   D(x): 0.6712, D(G(z)): 0.1310 / 0.5116\n",
      "Epoch [21/100] Batch 50/157                   Loss D: 0.8787, loss G: 0.9078                   D(x): 0.6194, D(G(z)): 0.0936 / 0.4465\n",
      "Epoch [21/100] Batch 100/157                   Loss D: 0.9353, loss G: 0.9151                   D(x): 0.6179, D(G(z)): 0.0939 / 0.4294\n",
      "Epoch [21/100] Batch 150/157                   Loss D: 0.9689, loss G: 0.8732                   D(x): 0.6257, D(G(z)): 0.1217 / 0.4358\n",
      "Epoch [22/100] Batch 0/157                   Loss D: 0.8167, loss G: 2.8018                   D(x): 0.8152, D(G(z)): 0.3902 / 0.0763\n",
      "Epoch [22/100] Batch 50/157                   Loss D: 0.9332, loss G: 2.5034                   D(x): 0.8253, D(G(z)): 0.4808 / 0.0943\n",
      "Epoch [22/100] Batch 100/157                   Loss D: 0.9736, loss G: 2.4182                   D(x): 0.7943, D(G(z)): 0.4673 / 0.1060\n",
      "Epoch [22/100] Batch 150/157                   Loss D: 0.8151, loss G: 2.4714                   D(x): 0.8434, D(G(z)): 0.4231 / 0.1045\n",
      "Epoch [23/100] Batch 0/157                   Loss D: 0.8880, loss G: 0.7934                   D(x): 0.6515, D(G(z)): 0.1251 / 0.4863\n",
      "Epoch [23/100] Batch 50/157                   Loss D: 0.8562, loss G: 0.9116                   D(x): 0.6253, D(G(z)): 0.0925 / 0.4407\n",
      "Epoch [23/100] Batch 100/157                   Loss D: 0.6817, loss G: 0.7600                   D(x): 0.7287, D(G(z)): 0.1038 / 0.4888\n",
      "Epoch [23/100] Batch 150/157                   Loss D: 1.0245, loss G: 0.8563                   D(x): 0.5716, D(G(z)): 0.0849 / 0.4540\n",
      "Epoch [24/100] Batch 0/157                   Loss D: 0.8867, loss G: 2.4407                   D(x): 0.8353, D(G(z)): 0.4596 / 0.1085\n",
      "Epoch [24/100] Batch 50/157                   Loss D: 0.7961, loss G: 2.4484                   D(x): 0.8667, D(G(z)): 0.4452 / 0.1150\n",
      "Epoch [24/100] Batch 100/157                   Loss D: 0.8075, loss G: 2.5899                   D(x): 0.7792, D(G(z)): 0.3889 / 0.0923\n",
      "Epoch [24/100] Batch 150/157                   Loss D: 0.8921, loss G: 2.4465                   D(x): 0.8795, D(G(z)): 0.4966 / 0.0987\n",
      "Epoch [25/100] Batch 0/157                   Loss D: 0.7899, loss G: 1.0865                   D(x): 0.7094, D(G(z)): 0.1517 / 0.3751\n",
      "Epoch [25/100] Batch 50/157                   Loss D: 0.7299, loss G: 0.9077                   D(x): 0.7156, D(G(z)): 0.0836 / 0.4278\n",
      "Epoch [25/100] Batch 100/157                   Loss D: 0.8986, loss G: 0.9348                   D(x): 0.6288, D(G(z)): 0.1120 / 0.4180\n",
      "Epoch [25/100] Batch 150/157                   Loss D: 0.8374, loss G: 1.0005                   D(x): 0.6813, D(G(z)): 0.0884 / 0.3926\n",
      "Epoch [26/100] Batch 0/157                   Loss D: 0.7667, loss G: 2.1216                   D(x): 0.7696, D(G(z)): 0.2866 / 0.1525\n",
      "Epoch [26/100] Batch 50/157                   Loss D: 0.8764, loss G: 2.5284                   D(x): 0.8352, D(G(z)): 0.4542 / 0.0937\n",
      "Epoch [26/100] Batch 100/157                   Loss D: 0.9675, loss G: 2.4928                   D(x): 0.8251, D(G(z)): 0.4797 / 0.1034\n",
      "Epoch [26/100] Batch 150/157                   Loss D: 0.8401, loss G: 2.4850                   D(x): 0.8465, D(G(z)): 0.4602 / 0.0984\n",
      "Epoch [27/100] Batch 0/157                   Loss D: 0.7243, loss G: 0.9025                   D(x): 0.7287, D(G(z)): 0.1296 / 0.4300\n",
      "Epoch [27/100] Batch 50/157                   Loss D: 0.8454, loss G: 0.9010                   D(x): 0.6692, D(G(z)): 0.1084 / 0.4220\n",
      "Epoch [27/100] Batch 100/157                   Loss D: 0.8426, loss G: 0.9330                   D(x): 0.6727, D(G(z)): 0.0822 / 0.4197\n",
      "Epoch [27/100] Batch 150/157                   Loss D: 0.7981, loss G: 0.9241                   D(x): 0.7031, D(G(z)): 0.1207 / 0.4270\n",
      "Epoch [28/100] Batch 0/157                   Loss D: 0.7506, loss G: 2.4635                   D(x): 0.8739, D(G(z)): 0.4218 / 0.1133\n",
      "Epoch [28/100] Batch 50/157                   Loss D: 0.7669, loss G: 2.5706                   D(x): 0.8551, D(G(z)): 0.4197 / 0.0853\n",
      "Epoch [28/100] Batch 100/157                   Loss D: 0.7498, loss G: 2.5506                   D(x): 0.8537, D(G(z)): 0.4126 / 0.0947\n",
      "Epoch [28/100] Batch 150/157                   Loss D: 0.9013, loss G: 2.4276                   D(x): 0.8198, D(G(z)): 0.4671 / 0.0958\n",
      "Epoch [29/100] Batch 0/157                   Loss D: 0.7627, loss G: 0.7547                   D(x): 0.7100, D(G(z)): 0.1448 / 0.4851\n",
      "Epoch [29/100] Batch 50/157                   Loss D: 0.6760, loss G: 0.8243                   D(x): 0.7222, D(G(z)): 0.1105 / 0.4647\n",
      "Epoch [29/100] Batch 100/157                   Loss D: 0.9941, loss G: 0.8639                   D(x): 0.6192, D(G(z)): 0.1152 / 0.4373\n",
      "Epoch [29/100] Batch 150/157                   Loss D: 0.9459, loss G: 0.9203                   D(x): 0.6401, D(G(z)): 0.1095 / 0.4161\n",
      "Epoch [30/100] Batch 0/157                   Loss D: 0.8582, loss G: 2.2674                   D(x): 0.8486, D(G(z)): 0.4699 / 0.1231\n",
      "Epoch [30/100] Batch 50/157                   Loss D: 0.7829, loss G: 2.5300                   D(x): 0.8740, D(G(z)): 0.4373 / 0.0917\n",
      "Epoch [30/100] Batch 100/157                   Loss D: 0.8188, loss G: 2.4571                   D(x): 0.8072, D(G(z)): 0.4035 / 0.0933\n",
      "Epoch [30/100] Batch 150/157                   Loss D: 0.8428, loss G: 2.4711                   D(x): 0.8227, D(G(z)): 0.4390 / 0.0912\n",
      "Epoch [31/100] Batch 0/157                   Loss D: 0.6763, loss G: 1.0890                   D(x): 0.7519, D(G(z)): 0.1995 / 0.3633\n",
      "Epoch [31/100] Batch 50/157                   Loss D: 1.0956, loss G: 0.9638                   D(x): 0.5553, D(G(z)): 0.0766 / 0.4028\n",
      "Epoch [31/100] Batch 100/157                   Loss D: 0.8758, loss G: 0.8850                   D(x): 0.6532, D(G(z)): 0.1055 / 0.4369\n",
      "Epoch [31/100] Batch 150/157                   Loss D: 0.9187, loss G: 0.8950                   D(x): 0.6188, D(G(z)): 0.0960 / 0.4214\n",
      "Epoch [32/100] Batch 0/157                   Loss D: 0.8047, loss G: 2.6160                   D(x): 0.8370, D(G(z)): 0.4270 / 0.0878\n",
      "Epoch [32/100] Batch 50/157                   Loss D: 0.8545, loss G: 2.4407                   D(x): 0.8308, D(G(z)): 0.4425 / 0.0991\n",
      "Epoch [32/100] Batch 100/157                   Loss D: 0.8510, loss G: 2.4409                   D(x): 0.8270, D(G(z)): 0.4452 / 0.1035\n",
      "Epoch [32/100] Batch 150/157                   Loss D: 0.7086, loss G: 2.5966                   D(x): 0.8535, D(G(z)): 0.3755 / 0.0829\n",
      "Epoch [33/100] Batch 0/157                   Loss D: 0.6328, loss G: 0.9905                   D(x): 0.7429, D(G(z)): 0.1137 / 0.3890\n",
      "Epoch [33/100] Batch 50/157                   Loss D: 0.8742, loss G: 0.9935                   D(x): 0.6743, D(G(z)): 0.0869 / 0.3886\n",
      "Epoch [33/100] Batch 100/157                   Loss D: 0.9458, loss G: 0.7996                   D(x): 0.6283, D(G(z)): 0.1087 / 0.4803\n",
      "Epoch [33/100] Batch 150/157                   Loss D: 0.8772, loss G: 0.9219                   D(x): 0.6295, D(G(z)): 0.1139 / 0.4147\n",
      "Epoch [34/100] Batch 0/157                   Loss D: 0.7277, loss G: 2.2143                   D(x): 0.7902, D(G(z)): 0.3341 / 0.1390\n",
      "Epoch [34/100] Batch 50/157                   Loss D: 0.8194, loss G: 2.4782                   D(x): 0.8453, D(G(z)): 0.4205 / 0.0946\n",
      "Epoch [34/100] Batch 100/157                   Loss D: 0.6954, loss G: 2.9985                   D(x): 0.8509, D(G(z)): 0.3405 / 0.0676\n",
      "Epoch [34/100] Batch 150/157                   Loss D: 0.7903, loss G: 2.5281                   D(x): 0.8719, D(G(z)): 0.4266 / 0.0959\n",
      "Epoch [35/100] Batch 0/157                   Loss D: 0.8208, loss G: 0.9383                   D(x): 0.6788, D(G(z)): 0.1016 / 0.4132\n",
      "Epoch [35/100] Batch 50/157                   Loss D: 0.7938, loss G: 0.8799                   D(x): 0.6971, D(G(z)): 0.1074 / 0.4276\n",
      "Epoch [35/100] Batch 100/157                   Loss D: 0.8546, loss G: 0.8879                   D(x): 0.6615, D(G(z)): 0.1087 / 0.4271\n",
      "Epoch [35/100] Batch 150/157                   Loss D: 0.6169, loss G: 1.0658                   D(x): 0.7652, D(G(z)): 0.0759 / 0.3660\n",
      "Epoch [36/100] Batch 0/157                   Loss D: 0.6359, loss G: 2.9000                   D(x): 0.8004, D(G(z)): 0.2740 / 0.0622\n",
      "Epoch [36/100] Batch 50/157                   Loss D: 0.7966, loss G: 2.4999                   D(x): 0.8320, D(G(z)): 0.3975 / 0.1012\n",
      "Epoch [36/100] Batch 100/157                   Loss D: 0.9045, loss G: 2.5221                   D(x): 0.7728, D(G(z)): 0.4209 / 0.0915\n",
      "Epoch [36/100] Batch 150/157                   Loss D: 0.7676, loss G: 2.4524                   D(x): 0.8973, D(G(z)): 0.4359 / 0.1074\n",
      "Epoch [37/100] Batch 0/157                   Loss D: 0.8742, loss G: 1.6313                   D(x): 0.7632, D(G(z)): 0.2371 / 0.2074\n",
      "Epoch [37/100] Batch 50/157                   Loss D: 0.8484, loss G: 1.0137                   D(x): 0.6712, D(G(z)): 0.0853 / 0.3876\n",
      "Epoch [37/100] Batch 100/157                   Loss D: 0.6621, loss G: 0.8550                   D(x): 0.7305, D(G(z)): 0.1051 / 0.4456\n",
      "Epoch [37/100] Batch 150/157                   Loss D: 0.7912, loss G: 0.8914                   D(x): 0.6796, D(G(z)): 0.0938 / 0.4307\n",
      "Epoch [38/100] Batch 0/157                   Loss D: 0.9171, loss G: 2.4134                   D(x): 0.8352, D(G(z)): 0.4827 / 0.1006\n",
      "Epoch [38/100] Batch 50/157                   Loss D: 0.7704, loss G: 2.6022                   D(x): 0.8200, D(G(z)): 0.4000 / 0.0888\n",
      "Epoch [38/100] Batch 100/157                   Loss D: 0.7899, loss G: 2.6035                   D(x): 0.7942, D(G(z)): 0.3827 / 0.0889\n",
      "Epoch [38/100] Batch 150/157                   Loss D: 0.8382, loss G: 2.5037                   D(x): 0.8390, D(G(z)): 0.4307 / 0.0913\n",
      "Epoch [39/100] Batch 0/157                   Loss D: 0.9132, loss G: 1.2617                   D(x): 0.6928, D(G(z)): 0.2565 / 0.3016\n",
      "Epoch [39/100] Batch 50/157                   Loss D: 0.7540, loss G: 0.9835                   D(x): 0.7248, D(G(z)): 0.0861 / 0.3879\n",
      "Epoch [39/100] Batch 100/157                   Loss D: 0.6760, loss G: 0.8942                   D(x): 0.7348, D(G(z)): 0.1179 / 0.4379\n",
      "Epoch [39/100] Batch 150/157                   Loss D: 0.8522, loss G: 0.8644                   D(x): 0.6895, D(G(z)): 0.1311 / 0.4423\n",
      "Epoch [40/100] Batch 0/157                   Loss D: 0.6896, loss G: 2.4466                   D(x): 0.8658, D(G(z)): 0.3639 / 0.0977\n",
      "Epoch [40/100] Batch 50/157                   Loss D: 0.7080, loss G: 2.7251                   D(x): 0.8517, D(G(z)): 0.3865 / 0.0768\n",
      "Epoch [40/100] Batch 100/157                   Loss D: 0.7164, loss G: 2.6013                   D(x): 0.8710, D(G(z)): 0.4049 / 0.0886\n",
      "Epoch [40/100] Batch 150/157                   Loss D: 0.8154, loss G: 2.4296                   D(x): 0.7740, D(G(z)): 0.3836 / 0.1038\n",
      "Epoch [41/100] Batch 0/157                   Loss D: 0.5730, loss G: 0.9228                   D(x): 0.7616, D(G(z)): 0.1095 / 0.4105\n",
      "Epoch [41/100] Batch 50/157                   Loss D: 0.5367, loss G: 1.0347                   D(x): 0.8043, D(G(z)): 0.0857 / 0.3708\n",
      "Epoch [41/100] Batch 100/157                   Loss D: 0.7704, loss G: 0.8570                   D(x): 0.6980, D(G(z)): 0.0830 / 0.4416\n",
      "Epoch [41/100] Batch 150/157                   Loss D: 0.8004, loss G: 1.0598                   D(x): 0.7020, D(G(z)): 0.0963 / 0.3807\n",
      "Epoch [42/100] Batch 0/157                   Loss D: 0.8107, loss G: 2.5080                   D(x): 0.8016, D(G(z)): 0.3952 / 0.0925\n",
      "Epoch [42/100] Batch 50/157                   Loss D: 0.8167, loss G: 2.5979                   D(x): 0.8052, D(G(z)): 0.4088 / 0.0858\n",
      "Epoch [42/100] Batch 100/157                   Loss D: 0.7263, loss G: 2.4419                   D(x): 0.8395, D(G(z)): 0.3899 / 0.0995\n",
      "Epoch [42/100] Batch 150/157                   Loss D: 0.7756, loss G: 2.4509                   D(x): 0.8816, D(G(z)): 0.4494 / 0.1055\n",
      "Epoch [43/100] Batch 0/157                   Loss D: 0.9676, loss G: 0.8321                   D(x): 0.6262, D(G(z)): 0.1247 / 0.4603\n",
      "Epoch [43/100] Batch 50/157                   Loss D: 0.6139, loss G: 0.9030                   D(x): 0.7582, D(G(z)): 0.0983 / 0.4342\n",
      "Epoch [43/100] Batch 100/157                   Loss D: 0.7968, loss G: 1.0387                   D(x): 0.6820, D(G(z)): 0.1238 / 0.3847\n",
      "Epoch [43/100] Batch 150/157                   Loss D: 0.6928, loss G: 1.0089                   D(x): 0.7336, D(G(z)): 0.1079 / 0.3889\n",
      "Epoch [44/100] Batch 0/157                   Loss D: 0.8405, loss G: 1.0930                   D(x): 0.7414, D(G(z)): 0.1736 / 0.3578\n",
      "Epoch [44/100] Batch 50/157                   Loss D: 0.6858, loss G: 0.9813                   D(x): 0.7411, D(G(z)): 0.0945 / 0.3986\n",
      "Epoch [44/100] Batch 100/157                   Loss D: 0.8840, loss G: 0.7060                   D(x): 0.6416, D(G(z)): 0.1118 / 0.5162\n",
      "Epoch [44/100] Batch 150/157                   Loss D: 0.7927, loss G: 0.9934                   D(x): 0.6921, D(G(z)): 0.0964 / 0.3884\n",
      "Epoch [45/100] Batch 0/157                   Loss D: 0.7690, loss G: 2.7808                   D(x): 0.8162, D(G(z)): 0.3832 / 0.0850\n",
      "Epoch [45/100] Batch 50/157                   Loss D: 0.9755, loss G: 2.0466                   D(x): 0.8397, D(G(z)): 0.5090 / 0.1553\n",
      "Epoch [45/100] Batch 100/157                   Loss D: 0.8794, loss G: 2.3927                   D(x): 0.8479, D(G(z)): 0.4468 / 0.1083\n",
      "Epoch [45/100] Batch 150/157                   Loss D: 0.9101, loss G: 1.0019                   D(x): 0.6421, D(G(z)): 0.1109 / 0.4024\n",
      "Epoch [46/100] Batch 0/157                   Loss D: 0.7228, loss G: 2.8101                   D(x): 0.7963, D(G(z)): 0.3280 / 0.0724\n",
      "Epoch [46/100] Batch 50/157                   Loss D: 0.9122, loss G: 2.1788                   D(x): 0.8780, D(G(z)): 0.5073 / 0.1286\n",
      "Epoch [46/100] Batch 100/157                   Loss D: 0.8083, loss G: 2.2990                   D(x): 0.8308, D(G(z)): 0.4225 / 0.1354\n",
      "Epoch [46/100] Batch 150/157                   Loss D: 0.8192, loss G: 1.9152                   D(x): 0.7720, D(G(z)): 0.3428 / 0.1912\n",
      "Epoch [47/100] Batch 0/157                   Loss D: 0.7598, loss G: 1.9558                   D(x): 0.7630, D(G(z)): 0.2610 / 0.1676\n",
      "Epoch [47/100] Batch 50/157                   Loss D: 0.8556, loss G: 2.6019                   D(x): 0.8501, D(G(z)): 0.4652 / 0.0889\n",
      "Epoch [47/100] Batch 100/157                   Loss D: 0.8510, loss G: 2.6781                   D(x): 0.8170, D(G(z)): 0.4294 / 0.0777\n",
      "Epoch [47/100] Batch 150/157                   Loss D: 0.7712, loss G: 2.5047                   D(x): 0.8787, D(G(z)): 0.4219 / 0.0990\n",
      "Epoch [48/100] Batch 0/157                   Loss D: 0.7247, loss G: 0.9470                   D(x): 0.7275, D(G(z)): 0.0960 / 0.4167\n",
      "Epoch [48/100] Batch 50/157                   Loss D: 1.1542, loss G: 0.9083                   D(x): 0.5442, D(G(z)): 0.1526 / 0.4364\n",
      "Epoch [48/100] Batch 100/157                   Loss D: 0.9793, loss G: 0.9966                   D(x): 0.6184, D(G(z)): 0.0936 / 0.4028\n",
      "Epoch [48/100] Batch 150/157                   Loss D: 0.6333, loss G: 0.8918                   D(x): 0.7536, D(G(z)): 0.0903 / 0.4359\n",
      "Epoch [49/100] Batch 0/157                   Loss D: 0.7725, loss G: 2.1400                   D(x): 0.8023, D(G(z)): 0.3782 / 0.1573\n",
      "Epoch [49/100] Batch 50/157                   Loss D: 1.0135, loss G: 2.2746                   D(x): 0.8407, D(G(z)): 0.5062 / 0.1229\n",
      "Epoch [49/100] Batch 100/157                   Loss D: 0.7926, loss G: 2.3229                   D(x): 0.8534, D(G(z)): 0.4132 / 0.1245\n",
      "Epoch [49/100] Batch 150/157                   Loss D: 0.8693, loss G: 2.4461                   D(x): 0.8407, D(G(z)): 0.4587 / 0.0975\n",
      "Epoch [50/100] Batch 0/157                   Loss D: 0.8466, loss G: 1.1153                   D(x): 0.6805, D(G(z)): 0.1189 / 0.3816\n",
      "Epoch [50/100] Batch 50/157                   Loss D: 0.7358, loss G: 0.9159                   D(x): 0.7191, D(G(z)): 0.0986 / 0.4313\n",
      "Epoch [50/100] Batch 100/157                   Loss D: 0.9781, loss G: 1.0350                   D(x): 0.6256, D(G(z)): 0.0878 / 0.3821\n",
      "Epoch [50/100] Batch 150/157                   Loss D: 0.7849, loss G: 0.9594                   D(x): 0.6666, D(G(z)): 0.1152 / 0.4159\n",
      "Epoch [51/100] Batch 0/157                   Loss D: 0.8522, loss G: 1.4372                   D(x): 0.7127, D(G(z)): 0.2306 / 0.2746\n",
      "Epoch [51/100] Batch 50/157                   Loss D: 0.7795, loss G: 0.9772                   D(x): 0.7026, D(G(z)): 0.0833 / 0.4015\n",
      "Epoch [51/100] Batch 100/157                   Loss D: 0.7631, loss G: 0.8541                   D(x): 0.7014, D(G(z)): 0.1158 / 0.4459\n",
      "Epoch [51/100] Batch 150/157                   Loss D: 0.8003, loss G: 0.6923                   D(x): 0.6844, D(G(z)): 0.1251 / 0.5256\n",
      "Epoch [52/100] Batch 0/157                   Loss D: 0.7947, loss G: 2.8411                   D(x): 0.8637, D(G(z)): 0.4266 / 0.0704\n",
      "Epoch [52/100] Batch 50/157                   Loss D: 0.8988, loss G: 2.5347                   D(x): 0.8452, D(G(z)): 0.4796 / 0.0879\n",
      "Epoch [52/100] Batch 100/157                   Loss D: 0.8367, loss G: 2.6644                   D(x): 0.8492, D(G(z)): 0.4425 / 0.0885\n",
      "Epoch [52/100] Batch 150/157                   Loss D: 0.7958, loss G: 2.7219                   D(x): 0.8787, D(G(z)): 0.4351 / 0.0758\n",
      "Epoch [53/100] Batch 0/157                   Loss D: 0.6949, loss G: 0.8816                   D(x): 0.7468, D(G(z)): 0.1300 / 0.4435\n",
      "Epoch [53/100] Batch 50/157                   Loss D: 0.8942, loss G: 0.8668                   D(x): 0.6506, D(G(z)): 0.1047 / 0.4447\n",
      "Epoch [53/100] Batch 100/157                   Loss D: 0.9399, loss G: 1.1395                   D(x): 0.6194, D(G(z)): 0.0818 / 0.3784\n",
      "Epoch [53/100] Batch 150/157                   Loss D: 0.5959, loss G: 1.0071                   D(x): 0.7678, D(G(z)): 0.0984 / 0.4045\n",
      "Epoch [54/100] Batch 0/157                   Loss D: 0.7442, loss G: 2.7002                   D(x): 0.8753, D(G(z)): 0.4250 / 0.0804\n",
      "Epoch [54/100] Batch 50/157                   Loss D: 0.9430, loss G: 2.3959                   D(x): 0.7877, D(G(z)): 0.4399 / 0.1043\n",
      "Epoch [54/100] Batch 100/157                   Loss D: 0.8713, loss G: 2.5244                   D(x): 0.8269, D(G(z)): 0.4414 / 0.0941\n",
      "Epoch [54/100] Batch 150/157                   Loss D: 0.7768, loss G: 2.4948                   D(x): 0.8667, D(G(z)): 0.4320 / 0.0952\n",
      "Epoch [55/100] Batch 0/157                   Loss D: 0.8083, loss G: 0.9187                   D(x): 0.6684, D(G(z)): 0.1177 / 0.4421\n",
      "Epoch [55/100] Batch 50/157                   Loss D: 0.9864, loss G: 0.9592                   D(x): 0.6017, D(G(z)): 0.0974 / 0.4198\n",
      "Epoch [55/100] Batch 100/157                   Loss D: 0.8303, loss G: 0.9364                   D(x): 0.6608, D(G(z)): 0.1093 / 0.4150\n",
      "Epoch [55/100] Batch 150/157                   Loss D: 0.8859, loss G: 0.8416                   D(x): 0.6464, D(G(z)): 0.0942 / 0.4577\n",
      "Epoch [56/100] Batch 0/157                   Loss D: 0.8238, loss G: 2.7722                   D(x): 0.8108, D(G(z)): 0.4088 / 0.0800\n",
      "Epoch [56/100] Batch 50/157                   Loss D: 0.7719, loss G: 2.5166                   D(x): 0.8197, D(G(z)): 0.3879 / 0.1056\n",
      "Epoch [56/100] Batch 100/157                   Loss D: 0.8470, loss G: 2.5171                   D(x): 0.8762, D(G(z)): 0.4836 / 0.0991\n",
      "Epoch [56/100] Batch 150/157                   Loss D: 0.8643, loss G: 2.3884                   D(x): 0.8361, D(G(z)): 0.4572 / 0.1003\n",
      "Epoch [57/100] Batch 0/157                   Loss D: 0.9980, loss G: 0.8848                   D(x): 0.6121, D(G(z)): 0.1363 / 0.4301\n",
      "Epoch [57/100] Batch 50/157                   Loss D: 0.8096, loss G: 0.9048                   D(x): 0.6518, D(G(z)): 0.0967 / 0.4301\n",
      "Epoch [57/100] Batch 100/157                   Loss D: 0.8694, loss G: 1.0487                   D(x): 0.6606, D(G(z)): 0.0846 / 0.3851\n",
      "Epoch [57/100] Batch 150/157                   Loss D: 0.7550, loss G: 0.9078                   D(x): 0.6923, D(G(z)): 0.0947 / 0.4334\n",
      "Epoch [58/100] Batch 0/157                   Loss D: 0.8834, loss G: 2.8611                   D(x): 0.7746, D(G(z)): 0.3997 / 0.0717\n",
      "Epoch [58/100] Batch 50/157                   Loss D: 0.7335, loss G: 2.6818                   D(x): 0.8635, D(G(z)): 0.4130 / 0.0809\n",
      "Epoch [58/100] Batch 100/157                   Loss D: 0.7598, loss G: 2.7995                   D(x): 0.8411, D(G(z)): 0.3949 / 0.0790\n",
      "Epoch [58/100] Batch 150/157                   Loss D: 0.7348, loss G: 3.0042                   D(x): 0.8178, D(G(z)): 0.3654 / 0.0627\n",
      "Epoch [59/100] Batch 0/157                   Loss D: 0.8699, loss G: 0.7942                   D(x): 0.6545, D(G(z)): 0.1160 / 0.4836\n",
      "Epoch [59/100] Batch 50/157                   Loss D: 0.9492, loss G: 0.9449                   D(x): 0.5933, D(G(z)): 0.1091 / 0.4210\n",
      "Epoch [59/100] Batch 100/157                   Loss D: 0.7466, loss G: 0.8168                   D(x): 0.7058, D(G(z)): 0.0971 / 0.4628\n",
      "Epoch [59/100] Batch 150/157                   Loss D: 0.7375, loss G: 0.8600                   D(x): 0.7018, D(G(z)): 0.0907 / 0.4520\n",
      "Epoch [60/100] Batch 0/157                   Loss D: 0.9163, loss G: 2.5006                   D(x): 0.8045, D(G(z)): 0.4459 / 0.1008\n",
      "Epoch [60/100] Batch 50/157                   Loss D: 0.7651, loss G: 2.5767                   D(x): 0.8555, D(G(z)): 0.4134 / 0.0972\n",
      "Epoch [60/100] Batch 100/157                   Loss D: 0.6696, loss G: 0.9435                   D(x): 0.7394, D(G(z)): 0.0955 / 0.4210\n",
      "Epoch [60/100] Batch 150/157                   Loss D: 0.9111, loss G: 0.8975                   D(x): 0.6399, D(G(z)): 0.1018 / 0.4370\n",
      "Epoch [61/100] Batch 0/157                   Loss D: 0.9681, loss G: 2.3852                   D(x): 0.7586, D(G(z)): 0.4268 / 0.1192\n",
      "Epoch [61/100] Batch 50/157                   Loss D: 0.8167, loss G: 2.5479                   D(x): 0.8370, D(G(z)): 0.4183 / 0.1015\n",
      "Epoch [61/100] Batch 100/157                   Loss D: 0.8487, loss G: 2.6506                   D(x): 0.8626, D(G(z)): 0.4711 / 0.0816\n",
      "Epoch [61/100] Batch 150/157                   Loss D: 0.8360, loss G: 2.5115                   D(x): 0.7895, D(G(z)): 0.3902 / 0.1017\n",
      "Epoch [62/100] Batch 0/157                   Loss D: 0.9128, loss G: 0.8606                   D(x): 0.6210, D(G(z)): 0.0992 / 0.4526\n",
      "Epoch [62/100] Batch 50/157                   Loss D: 0.8602, loss G: 0.8403                   D(x): 0.6440, D(G(z)): 0.1036 / 0.4588\n",
      "Epoch [62/100] Batch 100/157                   Loss D: 0.7085, loss G: 1.1347                   D(x): 0.7104, D(G(z)): 0.0963 / 0.3646\n",
      "Epoch [62/100] Batch 150/157                   Loss D: 1.0670, loss G: 0.9364                   D(x): 0.5805, D(G(z)): 0.0864 / 0.4192\n",
      "Epoch [63/100] Batch 0/157                   Loss D: 0.9405, loss G: 2.3598                   D(x): 0.8617, D(G(z)): 0.5053 / 0.1120\n",
      "Epoch [63/100] Batch 50/157                   Loss D: 0.7517, loss G: 2.7086                   D(x): 0.8623, D(G(z)): 0.4137 / 0.0838\n",
      "Epoch [63/100] Batch 100/157                   Loss D: 0.8101, loss G: 2.4795                   D(x): 0.8490, D(G(z)): 0.4420 / 0.0958\n",
      "Epoch [63/100] Batch 150/157                   Loss D: 0.7925, loss G: 2.7468                   D(x): 0.8270, D(G(z)): 0.4118 / 0.0781\n",
      "Epoch [64/100] Batch 0/157                   Loss D: 0.5811, loss G: 0.9012                   D(x): 0.7746, D(G(z)): 0.0842 / 0.4381\n",
      "Epoch [64/100] Batch 50/157                   Loss D: 0.7698, loss G: 1.4611                   D(x): 0.7378, D(G(z)): 0.1489 / 0.2823\n",
      "Epoch [64/100] Batch 100/157                   Loss D: 0.9369, loss G: 0.8485                   D(x): 0.6431, D(G(z)): 0.0986 / 0.4467\n",
      "Epoch [64/100] Batch 150/157                   Loss D: 1.0834, loss G: 0.8312                   D(x): 0.5908, D(G(z)): 0.1198 / 0.4509\n",
      "Epoch [65/100] Batch 0/157                   Loss D: 0.8516, loss G: 2.8094                   D(x): 0.7771, D(G(z)): 0.3900 / 0.0701\n",
      "Epoch [65/100] Batch 50/157                   Loss D: 0.8367, loss G: 2.5573                   D(x): 0.8361, D(G(z)): 0.4184 / 0.0882\n",
      "Epoch [65/100] Batch 100/157                   Loss D: 0.8513, loss G: 2.4921                   D(x): 0.8118, D(G(z)): 0.3989 / 0.1048\n",
      "Epoch [65/100] Batch 150/157                   Loss D: 0.7788, loss G: 2.5895                   D(x): 0.8468, D(G(z)): 0.4121 / 0.0941\n",
      "Epoch [66/100] Batch 0/157                   Loss D: 0.8153, loss G: 0.8759                   D(x): 0.6572, D(G(z)): 0.0848 / 0.4472\n",
      "Epoch [66/100] Batch 50/157                   Loss D: 0.9356, loss G: 0.8774                   D(x): 0.6466, D(G(z)): 0.0796 / 0.4401\n",
      "Epoch [66/100] Batch 100/157                   Loss D: 0.6473, loss G: 0.8482                   D(x): 0.7340, D(G(z)): 0.1013 / 0.4575\n",
      "Epoch [66/100] Batch 150/157                   Loss D: 1.0463, loss G: 0.9932                   D(x): 0.5759, D(G(z)): 0.0786 / 0.3969\n",
      "Epoch [67/100] Batch 0/157                   Loss D: 0.8230, loss G: 0.7223                   D(x): 0.6741, D(G(z)): 0.1958 / 0.5203\n",
      "Epoch [67/100] Batch 50/157                   Loss D: 0.6451, loss G: 0.8807                   D(x): 0.7448, D(G(z)): 0.1175 / 0.4617\n",
      "Epoch [67/100] Batch 100/157                   Loss D: 0.9312, loss G: 1.0983                   D(x): 0.6508, D(G(z)): 0.0814 / 0.3614\n",
      "Epoch [67/100] Batch 150/157                   Loss D: 0.9822, loss G: 2.3122                   D(x): 0.8427, D(G(z)): 0.5073 / 0.1183\n",
      "Epoch [68/100] Batch 0/157                   Loss D: 0.6285, loss G: 0.9109                   D(x): 0.7384, D(G(z)): 0.1121 / 0.4264\n",
      "Epoch [68/100] Batch 50/157                   Loss D: 0.8733, loss G: 0.9004                   D(x): 0.6407, D(G(z)): 0.1072 / 0.4383\n",
      "Epoch [68/100] Batch 100/157                   Loss D: 0.9340, loss G: 1.1072                   D(x): 0.6369, D(G(z)): 0.0839 / 0.3786\n",
      "Epoch [68/100] Batch 150/157                   Loss D: 0.7559, loss G: 0.9231                   D(x): 0.6985, D(G(z)): 0.1047 / 0.4180\n",
      "Epoch [69/100] Batch 0/157                   Loss D: 0.7626, loss G: 2.9216                   D(x): 0.7995, D(G(z)): 0.3589 / 0.0755\n",
      "Epoch [69/100] Batch 50/157                   Loss D: 0.9076, loss G: 2.4366                   D(x): 0.7879, D(G(z)): 0.4449 / 0.1004\n",
      "Epoch [69/100] Batch 100/157                   Loss D: 0.8978, loss G: 2.4317                   D(x): 0.8343, D(G(z)): 0.4641 / 0.1053\n",
      "Epoch [69/100] Batch 150/157                   Loss D: 0.8813, loss G: 2.6736                   D(x): 0.8147, D(G(z)): 0.4344 / 0.0881\n",
      "Epoch [70/100] Batch 0/157                   Loss D: 0.9276, loss G: 0.9583                   D(x): 0.6422, D(G(z)): 0.0863 / 0.4335\n",
      "Epoch [70/100] Batch 50/157                   Loss D: 0.9625, loss G: 0.8978                   D(x): 0.6191, D(G(z)): 0.1089 / 0.4335\n",
      "Epoch [70/100] Batch 100/157                   Loss D: 0.8207, loss G: 0.9144                   D(x): 0.6558, D(G(z)): 0.1049 / 0.4300\n",
      "Epoch [70/100] Batch 150/157                   Loss D: 0.8444, loss G: 1.0403                   D(x): 0.6598, D(G(z)): 0.1012 / 0.4107\n",
      "Epoch [71/100] Batch 0/157                   Loss D: 0.8308, loss G: 2.7117                   D(x): 0.8534, D(G(z)): 0.4207 / 0.0907\n",
      "Epoch [71/100] Batch 50/157                   Loss D: 0.8074, loss G: 2.4642                   D(x): 0.8625, D(G(z)): 0.4377 / 0.1008\n",
      "Epoch [71/100] Batch 100/157                   Loss D: 0.8907, loss G: 2.3369                   D(x): 0.8397, D(G(z)): 0.4736 / 0.1260\n",
      "Epoch [71/100] Batch 150/157                   Loss D: 0.9302, loss G: 2.4366                   D(x): 0.8002, D(G(z)): 0.4364 / 0.1066\n",
      "Epoch [72/100] Batch 0/157                   Loss D: 0.7098, loss G: 1.0623                   D(x): 0.6965, D(G(z)): 0.0785 / 0.3951\n",
      "Epoch [72/100] Batch 50/157                   Loss D: 0.7127, loss G: 0.8343                   D(x): 0.7166, D(G(z)): 0.1300 / 0.4654\n",
      "Epoch [72/100] Batch 100/157                   Loss D: 0.8692, loss G: 1.1062                   D(x): 0.6390, D(G(z)): 0.0916 / 0.3699\n",
      "Epoch [72/100] Batch 150/157                   Loss D: 0.7416, loss G: 0.8767                   D(x): 0.7129, D(G(z)): 0.1235 / 0.4463\n",
      "Epoch [73/100] Batch 0/157                   Loss D: 0.9400, loss G: 2.4899                   D(x): 0.8301, D(G(z)): 0.4558 / 0.0973\n",
      "Epoch [73/100] Batch 50/157                   Loss D: 0.7461, loss G: 2.5083                   D(x): 0.8751, D(G(z)): 0.4246 / 0.0887\n",
      "Epoch [73/100] Batch 100/157                   Loss D: 0.8365, loss G: 2.6003                   D(x): 0.8677, D(G(z)): 0.4576 / 0.0888\n",
      "Epoch [73/100] Batch 150/157                   Loss D: 0.7935, loss G: 2.4398                   D(x): 0.8217, D(G(z)): 0.3993 / 0.1241\n",
      "Epoch [74/100] Batch 0/157                   Loss D: 0.8022, loss G: 0.8509                   D(x): 0.6825, D(G(z)): 0.1108 / 0.4570\n",
      "Epoch [74/100] Batch 50/157                   Loss D: 0.7019, loss G: 0.9480                   D(x): 0.6997, D(G(z)): 0.1108 / 0.4126\n",
      "Epoch [74/100] Batch 100/157                   Loss D: 0.7449, loss G: 0.9079                   D(x): 0.7011, D(G(z)): 0.0925 / 0.4344\n",
      "Epoch [74/100] Batch 150/157                   Loss D: 0.9235, loss G: 1.0490                   D(x): 0.6482, D(G(z)): 0.0728 / 0.3841\n",
      "Epoch [75/100] Batch 0/157                   Loss D: 0.7185, loss G: 2.1599                   D(x): 0.8146, D(G(z)): 0.2818 / 0.1431\n",
      "Epoch [75/100] Batch 50/157                   Loss D: 0.7199, loss G: 2.6354                   D(x): 0.9127, D(G(z)): 0.4307 / 0.0895\n",
      "Epoch [75/100] Batch 100/157                   Loss D: 0.6883, loss G: 2.5840                   D(x): 0.8675, D(G(z)): 0.3871 / 0.0949\n",
      "Epoch [75/100] Batch 150/157                   Loss D: 0.8032, loss G: 2.5300                   D(x): 0.8399, D(G(z)): 0.4289 / 0.1061\n",
      "Epoch [76/100] Batch 0/157                   Loss D: 0.5918, loss G: 1.3021                   D(x): 0.7628, D(G(z)): 0.0957 / 0.3049\n",
      "Epoch [76/100] Batch 50/157                   Loss D: 0.8650, loss G: 1.0826                   D(x): 0.6630, D(G(z)): 0.0711 / 0.3571\n",
      "Epoch [76/100] Batch 100/157                   Loss D: 1.0934, loss G: 1.1229                   D(x): 0.6052, D(G(z)): 0.0766 / 0.3772\n",
      "Epoch [76/100] Batch 150/157                   Loss D: 0.9083, loss G: 0.9254                   D(x): 0.6403, D(G(z)): 0.1001 / 0.4377\n",
      "Epoch [77/100] Batch 0/157                   Loss D: 0.7861, loss G: 2.6704                   D(x): 0.8672, D(G(z)): 0.4145 / 0.0948\n",
      "Epoch [77/100] Batch 50/157                   Loss D: 0.8742, loss G: 2.5020                   D(x): 0.8501, D(G(z)): 0.4413 / 0.0982\n",
      "Epoch [77/100] Batch 100/157                   Loss D: 0.9051, loss G: 2.6758                   D(x): 0.8000, D(G(z)): 0.4576 / 0.0842\n",
      "Epoch [77/100] Batch 150/157                   Loss D: 0.7673, loss G: 2.4831                   D(x): 0.8163, D(G(z)): 0.3907 / 0.1182\n",
      "Epoch [78/100] Batch 0/157                   Loss D: 0.7033, loss G: 0.8970                   D(x): 0.7226, D(G(z)): 0.1373 / 0.4540\n",
      "Epoch [78/100] Batch 50/157                   Loss D: 0.7796, loss G: 0.9945                   D(x): 0.7106, D(G(z)): 0.1062 / 0.3990\n",
      "Epoch [78/100] Batch 100/157                   Loss D: 0.8663, loss G: 0.9490                   D(x): 0.6681, D(G(z)): 0.0769 / 0.4151\n",
      "Epoch [78/100] Batch 150/157                   Loss D: 0.7753, loss G: 0.8103                   D(x): 0.6914, D(G(z)): 0.1095 / 0.4732\n",
      "Epoch [79/100] Batch 0/157                   Loss D: 0.7152, loss G: 2.5304                   D(x): 0.7955, D(G(z)): 0.2956 / 0.1146\n",
      "Epoch [79/100] Batch 50/157                   Loss D: 0.8175, loss G: 2.2900                   D(x): 0.8127, D(G(z)): 0.3994 / 0.1545\n",
      "Epoch [79/100] Batch 100/157                   Loss D: 0.8652, loss G: 2.5982                   D(x): 0.8775, D(G(z)): 0.4616 / 0.0917\n",
      "Epoch [79/100] Batch 150/157                   Loss D: 0.8710, loss G: 2.3173                   D(x): 0.8484, D(G(z)): 0.4738 / 0.1155\n",
      "Epoch [80/100] Batch 0/157                   Loss D: 0.9135, loss G: 1.0546                   D(x): 0.6128, D(G(z)): 0.0834 / 0.3959\n",
      "Epoch [80/100] Batch 50/157                   Loss D: 0.7020, loss G: 0.9498                   D(x): 0.7223, D(G(z)): 0.1017 / 0.4223\n",
      "Epoch [80/100] Batch 100/157                   Loss D: 0.7719, loss G: 1.0011                   D(x): 0.6863, D(G(z)): 0.0897 / 0.4093\n",
      "Epoch [80/100] Batch 150/157                   Loss D: 0.7261, loss G: 0.8981                   D(x): 0.7194, D(G(z)): 0.1051 / 0.4431\n",
      "Epoch [81/100] Batch 0/157                   Loss D: 0.7194, loss G: 2.8862                   D(x): 0.8089, D(G(z)): 0.3429 / 0.0654\n",
      "Epoch [81/100] Batch 50/157                   Loss D: 0.7882, loss G: 2.4366                   D(x): 0.8355, D(G(z)): 0.4082 / 0.1111\n",
      "Epoch [81/100] Batch 100/157                   Loss D: 0.8361, loss G: 2.8414                   D(x): 0.8198, D(G(z)): 0.4267 / 0.0764\n",
      "Epoch [81/100] Batch 150/157                   Loss D: 0.8079, loss G: 2.5521                   D(x): 0.8230, D(G(z)): 0.4054 / 0.1021\n",
      "Epoch [82/100] Batch 0/157                   Loss D: 0.9512, loss G: 0.7416                   D(x): 0.6451, D(G(z)): 0.1777 / 0.5179\n",
      "Epoch [82/100] Batch 50/157                   Loss D: 0.7072, loss G: 1.8009                   D(x): 0.7854, D(G(z)): 0.2405 / 0.2053\n",
      "Epoch [82/100] Batch 100/157                   Loss D: 0.8927, loss G: 2.5768                   D(x): 0.7973, D(G(z)): 0.4360 / 0.0942\n",
      "Epoch [82/100] Batch 150/157                   Loss D: 0.8111, loss G: 2.5279                   D(x): 0.8196, D(G(z)): 0.3949 / 0.0879\n",
      "Epoch [83/100] Batch 0/157                   Loss D: 0.6788, loss G: 0.8149                   D(x): 0.7383, D(G(z)): 0.1296 / 0.4727\n",
      "Epoch [83/100] Batch 50/157                   Loss D: 0.8491, loss G: 0.9284                   D(x): 0.6691, D(G(z)): 0.0882 / 0.4380\n",
      "Epoch [83/100] Batch 100/157                   Loss D: 0.6326, loss G: 0.8749                   D(x): 0.7454, D(G(z)): 0.0941 / 0.4520\n",
      "Epoch [83/100] Batch 150/157                   Loss D: 0.8005, loss G: 0.8885                   D(x): 0.6742, D(G(z)): 0.0858 / 0.4370\n",
      "Epoch [84/100] Batch 0/157                   Loss D: 0.9100, loss G: 2.5087                   D(x): 0.8303, D(G(z)): 0.4567 / 0.0961\n",
      "Epoch [84/100] Batch 50/157                   Loss D: 0.7359, loss G: 2.7338                   D(x): 0.8695, D(G(z)): 0.4154 / 0.0792\n",
      "Epoch [84/100] Batch 100/157                   Loss D: 0.7641, loss G: 2.4444                   D(x): 0.8866, D(G(z)): 0.4379 / 0.1090\n",
      "Epoch [84/100] Batch 150/157                   Loss D: 0.8309, loss G: 2.7100                   D(x): 0.8337, D(G(z)): 0.4112 / 0.0763\n",
      "Epoch [85/100] Batch 0/157                   Loss D: 0.6677, loss G: 1.1706                   D(x): 0.7410, D(G(z)): 0.1157 / 0.3640\n",
      "Epoch [85/100] Batch 50/157                   Loss D: 1.0425, loss G: 0.8955                   D(x): 0.5812, D(G(z)): 0.0898 / 0.4337\n",
      "Epoch [85/100] Batch 100/157                   Loss D: 0.6391, loss G: 1.1156                   D(x): 0.7539, D(G(z)): 0.1184 / 0.3451\n",
      "Epoch [85/100] Batch 150/157                   Loss D: 0.8516, loss G: 2.4791                   D(x): 0.8410, D(G(z)): 0.4498 / 0.0997\n",
      "Epoch [86/100] Batch 0/157                   Loss D: 0.8386, loss G: 0.7342                   D(x): 0.6748, D(G(z)): 0.1608 / 0.5063\n",
      "Epoch [86/100] Batch 50/157                   Loss D: 0.8232, loss G: 0.8884                   D(x): 0.6576, D(G(z)): 0.0813 / 0.4458\n",
      "Epoch [86/100] Batch 100/157                   Loss D: 0.7728, loss G: 1.0379                   D(x): 0.6818, D(G(z)): 0.0969 / 0.3998\n",
      "Epoch [86/100] Batch 150/157                   Loss D: 0.7579, loss G: 0.9454                   D(x): 0.7272, D(G(z)): 0.1148 / 0.4240\n",
      "Epoch [87/100] Batch 0/157                   Loss D: 0.7086, loss G: 1.3264                   D(x): 0.7435, D(G(z)): 0.1867 / 0.3554\n",
      "Epoch [87/100] Batch 50/157                   Loss D: 1.1281, loss G: 1.0167                   D(x): 0.5741, D(G(z)): 0.0871 / 0.3969\n",
      "Epoch [87/100] Batch 100/157                   Loss D: 0.7074, loss G: 1.1197                   D(x): 0.7220, D(G(z)): 0.0988 / 0.3717\n",
      "Epoch [87/100] Batch 150/157                   Loss D: 0.8669, loss G: 1.0994                   D(x): 0.6655, D(G(z)): 0.1015 / 0.3766\n",
      "Epoch [88/100] Batch 0/157                   Loss D: 0.7160, loss G: 2.6393                   D(x): 0.8286, D(G(z)): 0.3550 / 0.0861\n",
      "Epoch [88/100] Batch 50/157                   Loss D: 0.8836, loss G: 2.4903                   D(x): 0.8111, D(G(z)): 0.4318 / 0.1071\n",
      "Epoch [88/100] Batch 100/157                   Loss D: 0.8114, loss G: 2.4925                   D(x): 0.8617, D(G(z)): 0.4483 / 0.1108\n",
      "Epoch [88/100] Batch 150/157                   Loss D: 0.7681, loss G: 2.6980                   D(x): 0.8510, D(G(z)): 0.3969 / 0.0995\n",
      "Epoch [89/100] Batch 0/157                   Loss D: 0.6915, loss G: 1.2162                   D(x): 0.7301, D(G(z)): 0.1816 / 0.3502\n",
      "Epoch [89/100] Batch 50/157                   Loss D: 0.7785, loss G: 0.9683                   D(x): 0.6576, D(G(z)): 0.0865 / 0.4150\n",
      "Epoch [89/100] Batch 100/157                   Loss D: 0.8286, loss G: 0.9915                   D(x): 0.6715, D(G(z)): 0.1023 / 0.4107\n",
      "Epoch [89/100] Batch 150/157                   Loss D: 0.9283, loss G: 0.8197                   D(x): 0.6153, D(G(z)): 0.0941 / 0.4742\n",
      "Epoch [90/100] Batch 0/157                   Loss D: 1.6305, loss G: 2.1545                   D(x): 0.5809, D(G(z)): 0.0238 / 0.1421\n",
      "Epoch [90/100] Batch 50/157                   Loss D: 0.9350, loss G: 2.4921                   D(x): 0.8119, D(G(z)): 0.4734 / 0.1179\n",
      "Epoch [90/100] Batch 100/157                   Loss D: 0.9982, loss G: 2.3931                   D(x): 0.8039, D(G(z)): 0.4839 / 0.1123\n",
      "Epoch [90/100] Batch 150/157                   Loss D: 0.8611, loss G: 2.7327                   D(x): 0.7900, D(G(z)): 0.4142 / 0.0856\n",
      "Epoch [91/100] Batch 0/157                   Loss D: 0.9861, loss G: 0.9333                   D(x): 0.6262, D(G(z)): 0.0741 / 0.4157\n",
      "Epoch [91/100] Batch 50/157                   Loss D: 0.7458, loss G: 0.9800                   D(x): 0.6821, D(G(z)): 0.1031 / 0.4201\n",
      "Epoch [91/100] Batch 100/157                   Loss D: 0.8605, loss G: 0.9780                   D(x): 0.6641, D(G(z)): 0.0745 / 0.4021\n",
      "Epoch [91/100] Batch 150/157                   Loss D: 0.6595, loss G: 0.8012                   D(x): 0.7269, D(G(z)): 0.1163 / 0.4808\n",
      "Epoch [92/100] Batch 0/157                   Loss D: 0.9482, loss G: 2.5030                   D(x): 0.7585, D(G(z)): 0.3801 / 0.1060\n",
      "Epoch [92/100] Batch 50/157                   Loss D: 0.8661, loss G: 2.5717                   D(x): 0.8267, D(G(z)): 0.4497 / 0.0945\n",
      "Epoch [92/100] Batch 100/157                   Loss D: 0.7648, loss G: 2.7946                   D(x): 0.8378, D(G(z)): 0.3834 / 0.0750\n",
      "Epoch [92/100] Batch 150/157                   Loss D: 0.8176, loss G: 2.5696                   D(x): 0.8593, D(G(z)): 0.4510 / 0.0892\n",
      "Epoch [93/100] Batch 0/157                   Loss D: 0.8998, loss G: 0.6647                   D(x): 0.6148, D(G(z)): 0.1448 / 0.5509\n",
      "Epoch [93/100] Batch 50/157                   Loss D: 0.8075, loss G: 1.0391                   D(x): 0.6937, D(G(z)): 0.1000 / 0.3828\n",
      "Epoch [93/100] Batch 100/157                   Loss D: 0.8405, loss G: 1.0640                   D(x): 0.6624, D(G(z)): 0.0719 / 0.3721\n",
      "Epoch [93/100] Batch 150/157                   Loss D: 0.9097, loss G: 2.5299                   D(x): 0.8032, D(G(z)): 0.4528 / 0.0921\n",
      "Epoch [94/100] Batch 0/157                   Loss D: 0.7276, loss G: 0.8256                   D(x): 0.7041, D(G(z)): 0.0970 / 0.4722\n",
      "Epoch [94/100] Batch 50/157                   Loss D: 0.8376, loss G: 1.0649                   D(x): 0.6725, D(G(z)): 0.0781 / 0.3868\n",
      "Epoch [94/100] Batch 100/157                   Loss D: 0.8762, loss G: 0.8945                   D(x): 0.6559, D(G(z)): 0.1303 / 0.4390\n",
      "Epoch [94/100] Batch 150/157                   Loss D: 0.8921, loss G: 0.8836                   D(x): 0.6530, D(G(z)): 0.0948 / 0.4303\n",
      "Epoch [95/100] Batch 0/157                   Loss D: 0.9810, loss G: 2.4547                   D(x): 0.8180, D(G(z)): 0.4888 / 0.1033\n",
      "Epoch [95/100] Batch 50/157                   Loss D: 1.0034, loss G: 2.2961                   D(x): 0.8267, D(G(z)): 0.5150 / 0.1182\n",
      "Epoch [95/100] Batch 100/157                   Loss D: 0.7578, loss G: 2.6910                   D(x): 0.8631, D(G(z)): 0.4165 / 0.0918\n",
      "Epoch [95/100] Batch 150/157                   Loss D: 0.8123, loss G: 2.4951                   D(x): 0.8302, D(G(z)): 0.4121 / 0.1067\n",
      "Epoch [96/100] Batch 0/157                   Loss D: 0.9417, loss G: 0.8959                   D(x): 0.6069, D(G(z)): 0.1063 / 0.4423\n",
      "Epoch [96/100] Batch 50/157                   Loss D: 0.9195, loss G: 0.8623                   D(x): 0.6254, D(G(z)): 0.0802 / 0.4555\n",
      "Epoch [96/100] Batch 100/157                   Loss D: 0.6612, loss G: 1.1752                   D(x): 0.7343, D(G(z)): 0.0902 / 0.3582\n",
      "Epoch [96/100] Batch 150/157                   Loss D: 0.6022, loss G: 0.9886                   D(x): 0.7652, D(G(z)): 0.0975 / 0.4025\n",
      "Epoch [97/100] Batch 0/157                   Loss D: 0.8171, loss G: 2.5023                   D(x): 0.8156, D(G(z)): 0.3983 / 0.1126\n",
      "Epoch [97/100] Batch 50/157                   Loss D: 0.7833, loss G: 2.7268                   D(x): 0.8576, D(G(z)): 0.4199 / 0.0833\n",
      "Epoch [97/100] Batch 100/157                   Loss D: 0.7736, loss G: 2.6421                   D(x): 0.8391, D(G(z)): 0.4020 / 0.0868\n",
      "Epoch [97/100] Batch 150/157                   Loss D: 0.8437, loss G: 2.4864                   D(x): 0.8789, D(G(z)): 0.4755 / 0.1089\n",
      "Epoch [98/100] Batch 0/157                   Loss D: 0.5902, loss G: 1.4373                   D(x): 0.8036, D(G(z)): 0.1125 / 0.2781\n",
      "Epoch [98/100] Batch 50/157                   Loss D: 0.7951, loss G: 2.8815                   D(x): 0.8169, D(G(z)): 0.3995 / 0.0767\n",
      "Epoch [98/100] Batch 100/157                   Loss D: 0.7736, loss G: 2.5433                   D(x): 0.8785, D(G(z)): 0.4255 / 0.1004\n",
      "Epoch [98/100] Batch 150/157                   Loss D: 0.9836, loss G: 2.4334                   D(x): 0.7862, D(G(z)): 0.4750 / 0.1266\n",
      "Epoch [99/100] Batch 0/157                   Loss D: 0.7346, loss G: 0.9036                   D(x): 0.7389, D(G(z)): 0.0982 / 0.4355\n",
      "Epoch [99/100] Batch 50/157                   Loss D: 0.8041, loss G: 1.1649                   D(x): 0.6620, D(G(z)): 0.0975 / 0.3889\n",
      "Epoch [99/100] Batch 100/157                   Loss D: 0.8535, loss G: 0.9559                   D(x): 0.6563, D(G(z)): 0.0825 / 0.4122\n",
      "Epoch [99/100] Batch 150/157                   Loss D: 0.7695, loss G: 0.9814                   D(x): 0.6984, D(G(z)): 0.0987 / 0.4152\n",
      "Epoch [100/100] Batch 0/157                   Loss D: 0.7382, loss G: 2.9908                   D(x): 0.8135, D(G(z)): 0.3384 / 0.0595\n",
      "Epoch [100/100] Batch 50/157                   Loss D: 0.7968, loss G: 2.5404                   D(x): 0.8340, D(G(z)): 0.3953 / 0.1061\n",
      "Epoch [100/100] Batch 100/157                   Loss D: 0.7931, loss G: 2.5433                   D(x): 0.8282, D(G(z)): 0.4116 / 0.0930\n",
      "Epoch [100/100] Batch 150/157                   Loss D: 0.8104, loss G: 2.1904                   D(x): 0.7899, D(G(z)): 0.3774 / 0.1294\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, (real_data, real_labels_one_hot) in enumerate(dataloader):\n",
    "        batch_size = real_data.size(0)\n",
    "        real_data = real_data.to(device)\n",
    "        real_labels_one_hot = real_labels_one_hot.to(device)\n",
    "\n",
    "\n",
    "        # Tạo nhãn thật và giả  \n",
    "        real_labels = torch.ones(batch_size, 1).to(device)  \n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "        \n",
    "        # ========================\n",
    "        #  Train Discriminator\n",
    "        # ========================\n",
    "        discriminator.zero_grad()\n",
    "        # Đánh giá dữ liệu thật với nhãn thật   \n",
    "        outputs = discriminator(real_data, real_labels_one_hot) \n",
    "        d_loss_real = nn.BCELoss()(outputs, real_labels)\n",
    "        d_loss_real.backward()\n",
    "        D_x = outputs.mean().item()\n",
    "        \n",
    "        # Tạo dữ liệu giả   \n",
    "        noise = torch.randn(batch_size, LATENT_DIM).to(device)  \n",
    "        random_labels = torch.randint(0, NUM_CLASSES, (batch_size,)).to(device) \n",
    "        random_labels_one_hot = torch.zeros(batch_size, NUM_CLASSES).to(device)\n",
    "        random_labels_one_hot.scatter_(1, random_labels.view(-1, 1), 1) \n",
    "        fake_data = generator(noise, random_labels_one_hot) \n",
    "        # Đánh giá dữ liệu giả với nhãn giả \n",
    "        outputs = discriminator(fake_data.detach(), random_labels_one_hot)\n",
    "        d_loss_fake = nn.BCELoss()(outputs, fake_labels)\n",
    "        d_loss_fake.backward()\n",
    "        D_G_z1 = outputs.mean().item()\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        # Cập nhật trọng số của Discriminator   \n",
    "        optimizerD = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))   \n",
    "        optimizerD.step()\n",
    "        # ========================  \n",
    "        #  Train Generator  \n",
    "        # ========================  \n",
    "        generator.zero_grad()   \n",
    "        # Tạo dữ liệu giả mới   \n",
    "        noise = torch.randn(batch_size, LATENT_DIM).to(device)  \n",
    "        random_labels = torch.randint(0, NUM_CLASSES, (batch_size,)).to(device) \n",
    "        random_labels_one_hot = torch.zeros(batch_size, NUM_CLASSES).to(device) \n",
    "        random_labels_one_hot.scatter_(1, random_labels.view(-1, 1), 1) \n",
    "        fake_data = generator(noise, random_labels_one_hot) \n",
    "        # Đánh giá dữ liệu giả với nhãn thật (muốn đánh lừa Discriminator)  \n",
    "        outputs = discriminator(fake_data, random_labels_one_hot)   \n",
    "        g_loss = nn.BCELoss()(outputs, real_labels) \n",
    "        g_loss.backward()   \n",
    "        D_G_z2 = outputs.mean().item()  \n",
    "        # Cập nhật trọng số của Generator   \n",
    "        optimizerG = optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))   \n",
    "        optimizerG.step()   \n",
    "        if i % 50 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Batch {i}/{len(dataloader)} \\\n",
    "                  Loss D: {d_loss.item():.4f}, loss G: {g_loss.item():.4f} \\\n",
    "                  D(x): {D_x:.4f}, D(G(z)): {D_G_z1:.4f} / {D_G_z2:.4f}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cbba4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Đang tạo 5 mẫu cho nhãn: 'index' (int: 0)\n",
      "\n",
      "Đang tạo 5 mẫu cho nhãn: 'thumb' (int: 1)\n"
     ]
    }
   ],
   "source": [
    "fake_data_with_labels = {}\n",
    "data = []\n",
    "# Lặp qua từng nhãn dán có trong dataset gốc\n",
    "num_samples_per_label = 5  # Số mẫu muốn tạo cho mỗi nhãn\n",
    "for label, label_int in dataset.label_to_int.items():\n",
    "    print(f\"\\nĐang tạo {num_samples_per_label} mẫu cho nhãn: '{label}' (int: {label_int})\")\n",
    "    \n",
    "    # 1. Tạo vector nhãn dán ở dạng one-hot\n",
    "    # Tạo tensor nhãn integer\n",
    "    target_label_int_tensor = torch.full((num_samples_per_label,), label_int, dtype=torch.long, device=device)\n",
    "    # Chuyển đổi thành one-hot encoding\n",
    "    target_labels_one_hot = torch.nn.functional.one_hot(target_label_int_tensor, num_classes=NUM_CLASSES    ).float().to(device)\n",
    "\n",
    "    # 2. Tạo vector nhiễu ngẫu nhiên\n",
    "    noise = torch.randn(num_samples_per_label, LATENT_DIM   , device=device)\n",
    "\n",
    "    # 3. Sử dụng Generator để tạo dữ liệu giả\n",
    "    with torch.no_grad():\n",
    "        fake_coords_normalized = generator(noise, target_labels_one_hot).cpu().numpy()\n",
    "\n",
    "    # 4. Khôi phục tọa độ về tỷ lệ gốc\n",
    "    fake_coords_original_scale = (fake_coords_normalized + 1) / 2 * (dataset.coordinates_max - dataset.coordinates_min) + dataset.coordinates_min\n",
    "\n",
    "    # 5. Lưu dữ liệu giả và nhãn tương ứng vào dictionary\n",
    "    # Lặp lại nhãn để khớp với số lượng mẫu đã tạo\n",
    "    fake_labels_np = np.full((num_samples_per_label, 1), label)\n",
    "    \n",
    "    # Kết hợp tọa độ giả và nhãn giả\n",
    "    combined_data = np.hstack((fake_coords_original_scale, fake_labels_np))\n",
    "    \n",
    "    fake_data_with_labels[label] = combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "519e2930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 22) (5, 21)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must be the same size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[104], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m ax \u001b[38;5;241m=\u001b[39m axes[i]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Vẽ các điểm mốc (landmarks)\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_coords\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_coords\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mblue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzorder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Vẽ các đường nối giữa các điểm\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start, end \u001b[38;5;129;01min\u001b[39;00m connections:\n",
      "File \u001b[1;32md:\\GANs\\myenv\\lib\\site-packages\\matplotlib\\__init__.py:1476\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1473\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1474\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1475\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1476\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\n\u001b[0;32m   1477\u001b[0m             ax,\n\u001b[0;32m   1478\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(sanitize_sequence, args),\n\u001b[0;32m   1479\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: sanitize_sequence(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[0;32m   1481\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1482\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1483\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32md:\\GANs\\myenv\\lib\\site-packages\\matplotlib\\axes\\_axes.py:4787\u001b[0m, in \u001b[0;36mAxes.scatter\u001b[1;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[0;32m   4785\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mma\u001b[38;5;241m.\u001b[39mravel(y)\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39msize:\n\u001b[1;32m-> 4787\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must be the same size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4790\u001b[0m     s \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_internal.classic_mode\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[0;32m   4791\u001b[0m          mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlines.markersize\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2.0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must be the same size"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Giả định dữ liệu là 21 điểm của bàn tay (42 tọa độ)\n",
    "num_landmarks = 42 // 2\n",
    "\n",
    "# Định nghĩa các kết nối giữa các điểm để vẽ đường nối\n",
    "# Dựa trên cấu trúc bàn tay của MediaPipe Hand Landmarks\n",
    "connections = [\n",
    "    (0, 1), (1, 2), (2, 3), (3, 4),      # Ngón cái\n",
    "    (0, 5), (5, 6), (6, 7), (7, 8),      # Ngón trỏ\n",
    "    (5, 9), (9, 10), (10, 11), (11, 12), # Ngón giữa\n",
    "    (9, 13), (13, 14), (14, 15), (15, 16), # Ngón áp út\n",
    "    (13, 17), (17, 18), (18, 19), (19, 20), # Ngón út\n",
    "    (0, 17) # Nối ngón cái và ngón út\n",
    "]\n",
    "\n",
    "# Tạo một figure và các subplot để hiển thị 5 mẫu\n",
    "fig, axes = plt.subplots(1, num_samples_per_label*NUM_CLASSES, figsize=(15, 5))\n",
    "fig.suptitle('Mẫu dữ liệu tọa độ MediaPipe được tạo bởi GAN', fontsize=16)\n",
    "for key, value in fake_data_with_labels.items():\n",
    "    x_coords = value[:, 0::2]  # Tọa độ x   \n",
    "    y_coords = value[:, 1::2]  # Tọa độ y\n",
    "    item = value[0]\n",
    "    print(x_coords.shape, y_coords.shape)   \n",
    "    \n",
    "    for i in range(num_samples_per_label):  \n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Vẽ các điểm mốc (landmarks)\n",
    "        ax.scatter(x_coords[i], y_coords[i], color='blue', s=20, zorder=5)\n",
    "\n",
    "        # Vẽ các đường nối giữa các điểm\n",
    "        for start, end in connections:\n",
    "            ax.plot([x_coords[i][start], x_coords[i][end]], \n",
    "                    [y_coords[i][start], y_coords[i][end]], \n",
    "                    color='green', linewidth=2, zorder=1)\n",
    "\n",
    "        # Đặt giới hạn cho trục để hình ảnh không bị méo\n",
    "        # Matplotlib có thể tự động điều chỉnh giới hạn, nhưng việc đặt thủ công giúp các subplot đồng nhất\n",
    "        ax.set_xlim(dataset.coordinates_min[0::2].min(), dataset.coordinates_max[0::2].max())\n",
    "        ax.set_ylim(dataset.coordinates_min[1::2].min(), dataset.coordinates_max[1::2].max())\n",
    "\n",
    "        # Đảo ngược trục y để gốc tọa độ ở trên cùng, giống với hình ảnh thông thường\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_title(f'Mẫu {i+1} - Nhãn: {item[0]}')\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "# for i in range(num_samples_per_label):\n",
    "#     # Trích xuất tọa độ x, y cho mẫu thứ i\n",
    "#     x_coords = data[i][i, 0::2]\n",
    "#     y_coords = data[i][i, 1::2]\n",
    "#     print (x_coords)\n",
    "#     print(\"/\")\n",
    "#     print (y_coords)    \n",
    "    \n",
    "#     ax = axes[i]\n",
    "    \n",
    "#     # Vẽ các điểm mốc (landmarks)\n",
    "#     ax.scatter(x_coords, y_coords, color='blue', s=20, zorder=5)\n",
    "\n",
    "#     # Vẽ các đường nối giữa các điểm\n",
    "#     for start, end in connections:\n",
    "#         ax.plot([x_coords[start], x_coords[end]], \n",
    "#                 [y_coords[start], y_coords[end]], \n",
    "#                 color='green', linewidth=2, zorder=1)\n",
    "\n",
    "#     # Đặt giới hạn cho trục để hình ảnh không bị méo\n",
    "#     # Matplotlib có thể tự động điều chỉnh giới hạn, nhưng việc đặt thủ công giúp các subplot đồng nhất\n",
    "#     ax.set_xlim(dataset.coordinates_min[0::2].min(), dataset.coordinates_max[0::2].max())\n",
    "#     ax.set_ylim(dataset.coordinates_min[1::2].min(), dataset.coordinates_max[1::2].max())\n",
    "\n",
    "#     # Đảo ngược trục y để gốc tọa độ ở trên cùng, giống với hình ảnh thông thường\n",
    "#     ax.invert_yaxis()\n",
    "#     ax.set_title(f'Mẫu {i+1}')\n",
    "#     ax.set_aspect('equal', adjustable='box') # Đảm bảo tỷ lệ x/y đồng nhất\n",
    "\n",
    "# plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f15caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e1ad27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
